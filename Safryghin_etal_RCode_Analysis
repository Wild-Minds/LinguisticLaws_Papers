#Zipf's law analysis----
#Correlation anaylsis between f and d
GestureTypes <- read.csv(file.choose(), header = T) #Load data
cor.test(GestureTypes$F, GestureTypes$d, method = 'spearman', alternative='greater') #correlation between f and d
cor.test(GestureTypes$F, GestureTypes$D, method = 'spearman') #Control correlation between D and f


#Excluding the outlier object shake
FreqeuncySort<-GestureTypes[order(-GestureTypes$F),]
GestureTypesNoOS <-FreqeuncySort[2:26,]

cor.test(GestureTypesNoOS$F, GestureTypesNoOS$d, method = 'spearman', alternative ='greater') #correlation between f and d
cor.test(GestureTypesNoOS$F, GestureTypesNoOS$D, method = 'spearman') #Control correlation between D and f


#Is L significnatly small for gesture types? from Heesen et al. (2019)
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypes$p)
y <- c(GestureTypes$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Is L significnatly small when excluding the outlier?
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypesNoOS$p)
y <- c(GestureTypesNoOS$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Menzerath's law analysis ----
GestureTokens<- read.csv(file.choose(), header=T)
cor.test(GestureTokens$Size, GestureTokens$t, method = 'spearman') #Correlation between sequence size and mean gesture duration per each sequence
cor.test(GestureTokens$Size, GestureTokens$T, method = 'spearman', alternative='greater') #Control correlation between T and n


#is M significanlty small?
reps <- 100000
results <- rep(0, reps)
x <- c(GTMixed$Size)
y <- c(GTMixed$t)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))

#GLMM analysis ---- 

#load packages 
library(lme4)
library(Matrix)
library(MuMIn)
library(car)
library(carData)
library(multcomp)

#Load data
GLMMData <-read.csv(file.choose(), header=T)

#Data distribution analysis----
hist(GLMMDataMAU$Duration)
#Test distribution data
#install.packages("fitdistrplus")
library(fitdistrplus)
citation('fitdistrplus')
#I start by plotting the empirical distribution
plotdist(GLMMData$Duration, histo=TRUE, demp=T)

#now I will look at the skewness and kurtosis of the distribution
par(mfrow = c(1,1))
descdist(GLMMData$Duration, boot=1000)

#fitting first weibull distribution
fitW <- fitdist(GLMMData$Duration, "weibull")
summary(fitW)

#fitting second gamma distribution
fitG <- fitdist(GLMMData$Duration, "gamma")
summary(fitG)

#try also lognormal
fitN <- fitdist(GLMMData$Duration, "lnorm")
summary(fitN)

#create more plots to compare the fit via weibull and gamma
par(mfrow = c(1,1))
plot.legend <- c("Weibull", "gamma", "lognorm")
denscomp(list(fitW, fitG, fitN))
qqcomp(list(fitW,fitG, fitN))
cdfcomp(list(fitW,fitG, fitN))
ppcomp(list(fitW,fitG, fitN))

#more goodness of fit tests:
gofstat(list(fitW, fitG, fitN), fitnames = c("weibull", "gamma", "lognormal"))

#Transformation of data
GLMMData$logduration<-log(GLMMData$Duration) #transform duration in log


#Model 1 
Model1 <- lmer(logduration ~ P + Category + P*Category+(1|Signaller), 
                data=GLMMData, na.action=na.fail)
summary(Model1)
confint(Model1)


ModelRankingAIC <-dredge(modelx, rank = 'AIC')
ModelRankingBIC <-dredge(modelx, rank = 'BIC')



#Model 2
Model2Data<-read.csv(file.choose(), header=T)
Model2 <- lmer(logduration ~ P + Category + P*Category+(1|Signaller), 
                           data=Model2Data, na.action=na.fail)
summary(Model2)
confint(Model2)

Model2AIC <-dredge(Model2, rank = 'AIC')
Model2BIC <- dredge(Model2, rank='BIC')

#Model 3 - Menzerath's law
GestureTokens<-read.csv(file.choose(), header=T)
Model3 <- lmer(logduration ~ PWB + Size + PWB*Size+(1|Signaller), 
                           data=GestureTokens, na.action = na.fail)
summary(Model3)
confint(Model3)

Model3AIC <-dredge(Model3, rank = 'AIC')
Model3BIC <- dredge(Model3, rank='BIC')


#Duane analysis ----
NoDuane <-subset(GestureTokens, !Signaller == 'Duane' )

cor.test(NoDuane$Size, NoDuane$t, method = 'spearman', alternative='greater') #Correlation between sequence size and mean gesture duration per each sequence
cor.test(NoDuane$Size, NoDuane$T, method = 'spearman', alternative='greater') #Control correlation between T and n


#Is M significantly small for sequences without Duane?
reps <- 100000
results <- rep(0, reps)
x <- c(NoDuane$Size)
y <- c(NoDuane$t)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results<M)
print(c("P of being so small is estimated as ", is_small/reps))

#GLMM
library(lme4)
library(MuMIn)
Model4<- lmer(logduration ~ PWB + Size + PWB*Size+(1|Signaller), 
               data=NoDuane, na.action = na.fail)
summary(Model4)
confint(Model4)

