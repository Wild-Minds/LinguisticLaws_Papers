#Zipf's law analysis----
#Correlation anaylsis between f and d
GestureTypes <- read.csv(file.choose(), header = T) #Load data
cor.test(GestureTypes$F, GestureTypes$d, method = 'spearman', alternative='greater') #correlation between f and d
cor.test(GestureTypes$F, GestureTypes$D, method = 'spearman') #Control correlation between D and f


#Excluding the outlier object shake
FreqeuncySort<-GestureTypes[order(-GestureTypes$F),]
GestureTypesNoOS <-FreqeuncySort[2:26,]

cor.test(GestureTypesNoOS$F, GestureTypesNoOS$d, method = 'spearman', alternative ='greater') #correlation between f and d
cor.test(GestureTypesNoOS$F, GestureTypesNoOS$D, method = 'spearman') #Control correlation between D and f


#Is L significnatly small for gesture types? from Heesen et al. (2019)
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypes$p)
y <- c(GestureTypes$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Is L significnatly small when excluding the outlier?
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypesNoOS$p)
y <- c(GestureTypesNoOS$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Menzerath's law analysis ----
GestureTokens<- read.csv(file.choose(), header=T)
cor.test(GestureTokens$Size, GestureTokens$d, method = 'spearman') #Correlation between sequence size and mean gesture duration per each sequence
cor.test(GestureTokens$Size, GestureTokens$D, method = 'spearman', alternative='greater') #Control correlation between T and n


#is M significanlty small?
reps <- 100000
results <- rep(0, reps)
x <- c(GTMixed$Size)
y <- c(GTMixed$d)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))

#GLMM analysis ---- 

#load packages 
library(lme4)
library(Matrix)
library(MuMIn)
library(car)
library(carData)
library(multcomp)

#Load data
GLMMData <-read.csv(file.choose(), header=T)
GLMMData$logduration<-log(GLMMData$Duration) #transform duration in log


#Model 1 
Model1 <- lmer(logduration ~ P + Category + P*Category+(1|Signaller), 
                data=GLMMData, na.action=na.fail)
summary(Model1)
confint(Model1)


ModelRankingAIC <-dredge(modelx, rank = 'AIC')
ModelRankingBIC <-dredge(modelx, rank = 'BIC')



#Model 2
Model2Data<-read.csv(file.choose(), header=T)
Model2 <- lmer(logduration ~ P + Category + P*Category+(1|Signaller), 
                           data=Model2Data, na.action=na.fail)
summary(Model2)
confint(Model2)

Model2AIC <-dredge(Model2, rank = 'AIC')
Model2BIC <- dredge(Model2, rank='BIC')

#Model 3 - Menzerath's law
GestureTokens<-read.csv(file.choose(), header=T)
Model3 <- lmer(logduration ~ PWB + Size + PWB*Size+(1|Signaller), 
                           data=GestureTokens, na.action = na.fail)
summary(Model3)
confint(Model3)

Model3AIC <-dredge(Model3, rank = 'AIC')
Model3BIC <- dredge(Model3, rank='BIC')



