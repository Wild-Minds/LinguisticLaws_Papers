#Zipf's law analysis----
#Correlation anaylsis between f and d
GestureTypes <- read.csv(file.choose(), header = T) #Load data
GestureTypes
cor.test(GestureTypes$F, GestureTypes$d, method = 'spearman', alternative='greater') #correlation between f and d
cor.test(GestureTypes$F, GestureTypes$D, method = 'spearman') #Control correlation between D and f


#Excluding the outlier object shake
FreqeuncySort<-GestureTypes[order(-GestureTypes$F),]
GestureTypesNoOS <-FreqeuncySort[2:26,]
GestureTypesNoOS
cor.test(GestureTypesNoOS$F, GestureTypesNoOS$d, method = 'spearman', alternative ='greater') #correlation between f and d
cor.test(GestureTypesNoOS$F, GestureTypesNoOS$D, method = 'spearman') #Control correlation between D and f


#Is L significnatly small for gesture types? from Heesen et al. (2019)
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypes$p)
y <- c(GestureTypes$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Is L significnatly small when excluding the outlier?
reps <- 100000
results <- rep(0, reps)
x <- c(GestureTypesNoOS$p)
y <- c(GestureTypesNoOS$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #changed check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 


#subset analysis manual vs whole body
ManualBody <-GestureTypes[order(GestureTypes$Type),]
ManualBody
Body <- ManualBody[1:5,]
Body
cor.test(Body$F, Body$d, method = 'spearman', alternative='less') #correlation between f and d
cor.test(Body$F, Body$d, method = 'spearman') #Control correlation between D and f

Manual <- ManualBody[6:26,]
cor.test(Manual$F, Manual$d, method = 'spearman', alternative = 'greater') #correlation between f and d
cor.test(Manual$F, Manual$d, method = 'spearman') #Control correlation between D and f


#Is L significantly small for manual gestures?
reps <- 100000
results <- rep(0, reps)
x <- c(Manual$p)
y <- c(Manual$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 

#Is L significantly small for body gestures?
reps <- 100000
results <- rep(0, reps)
x <- c(Body$p)
y <- c(Body$d)
L <- sum(x*y)
print (c("real L is", L))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  L_temp <- sum(xtemp *y)
  results[i] <- L_temp
}
hist(results)
is_small <- sum(results <L) #check if it is significiantly big swap '<' with '>'
print(c("P of being so small is estimated as ", is_small/reps)) 


#Menzerath's law analysis ----
GestureTokens<- read.csv(file.choose(), header=T)
head(GestureTokens)
cor.test(GestureTokens$Size, GestureTokens$d, method = 'spearman') #Correlation between sequence size and mean gesture duration per each sequence
cor.test(GestureTokens$Size, GestureTokens$D, method = 'spearman', alternative='greater') #Control correlation between T and n
GestureTokens

#is M significanlty small?
reps <- 100000
results <- rep(0, reps)
x <- c(GTMixed$Size)
y <- c(GTMixed$d)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))

#Subset Menzerath's law analysis 
#Manual sequences
GTManual <-subset(GestureTokens, Type=='manual')
head(GTManual)
cor.test(GTManual$Size, GTManual$d, method = 'spearman', alternative='greater') #Correlation between t and n per each sequence
cor.test(GTManual$Size, GTManual$D, method = 'spearman') #Control correlation between T and n

#Is M significanlty small for manual seqeuences?
reps <- 100000
results <- rep(0, reps)
x <- c(GTManual$Size)
y <- c(GTManual$d)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))


#Mixed sequences
GTMixed <-subset(GestureTokens, Type=='Mixed')
cor.test(GTMixed$Size, GTMixed$d, method = 'spearman', alternative = 'greater')#Correlation between t and n per each sequence
cor.test(GTMixed$Size, GTMixed$D, method = 'spearman')  #Control correlation between T and n

#Is M significantly small for mixed sequences?
reps <- 100000
results <- rep(0, reps)
x <- c(GTMixed$Size)
y <- c(GTMixed$d)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))

#Whole body sequences subset analysis
GTBody <-subset(GestureTokens, Type=='Body')
cor.test(GTBody$Size, GTBody$d, method = 'spearman', alternative = 'greater') #Correlation between t and n per each sequence
cor.test(GTBody$Size, GTBody$D, method = 'spearman') #Control correlation between T and n

#Is M significanlty small for mixed sequences?
reps <- 100000
results <- rep(0, reps)
x <- c(GTBody$Size)
y <- c(GTBody$d)
M <- sum(x*y)
print (c("real M is", M))
sortvector <- 1:length(x)
for (i in 1:reps)
{
  sortvector <- sample(sortvector, replace = F)
  xtemp <- x[sortvector]
  M_temp <- sum(xtemp *y)
  results[i] <- M_temp
}
hist(results)
is_small <- sum(results 
                >M)
print(c("P of being so small is estimated as ", is_small/reps))


#GLMM analysis ---- 

#load packages 
library(lme4)
library(Matrix)
library(MuMIn)
library(car)
library(carData)
library(multcomp)

#Load data
GLMMData <-read.csv(file.choose(), header=T)

#Model 1 
Model1 <- glmer(Duration ~ p + Type +(1|Signaller), 
                              data=GLMMData, family = Gamma (log))
options(na.action=na.pass) 
summary(Model1)

vif(Model1)
ModelRankingAIC <-dredge(Model1, rank = 'AIC')
ModelRankingBIC <-dredge(Model1, rank = 'BIC')
confint(Model1, c('p', 'Type'))

#posthoc
summary(glht(Model1, mcp(Context="Tukey")))


#No object shake Model 1 
GLMMNoOSData <- subset(GLMMData, Gesture != 'Object shake')
Model1NOOS <- glmer(Duration ~ p + Type +(1|Signaller), 
                           data=GLMMNoOSData, family = Gamma (log))
options(na.action=na.pass) 
summary(Model1NOOS)
confint(Model1NOOS)

Model1NOOSAIC <-dredge(Model1NOOS, rank = 'AIC')
Model1NOOSBIC <- dredge(Model1NOOS, rank='BIC')

#Menzerath's GLMM 
Model2 <- glmer(d ~ Type + Size + (1|Signaller), 
                           data=GestureTokens, family = Gamma (log))
options(na.action=na.pass) 
summary(Model2)
confint(Model2)

Model2AIC <-dredge(Model2, rank = 'AIC')
Model2BIC <- dredge(Model2, rank='BIC')


#post hoc
summary(glht(Model2, mcp(Type="Tukey")))



